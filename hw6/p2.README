/*   Group info:
 *   kmishra Kushagra Mishra
 *   pranjan Pritesh Ranjan
 *   aapatel8 Akshit Patel
 */
 // Extra credit assignment after RAW DATA section.
Q. Compare the execution time of your lake.py against lake.o using the parameters 
    N=512, npebs=40, num_iter=400.
------------
A. 
 lake.py execution times.

 --- GTX 480. Compute capability 2.0
 Since, this GPU has compute capability < 3.0, it is not utilized by tensorflow and 
 the program is executed on CPU. 

 [pranjan@c7 hw6]$ ./lake.py 512 40 400
2017-12-10 01:02:23.071613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 480 major: 2 minor: 0 memoryClockRate(GHz): 1.401
pciBusID: 0000:03:00.0
totalMemory: 1.44GiB freeMemory: 1.39GiB
2017-12-10 01:02:23.072807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093] Ignoring visible gpu device (device: 0, name: GeForce GTX 480, pci bus id: 0000:03:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.
Elapsed time: 7.30864596367 seconds

lake.o execution time.
[pranjan@c7 hw6]$ ./lake.o 512 40 400
Running ./lake.o with (512 x 512) grid, until 400 iterations, with 16 threads
Main loop took 0.551862 seconds

 As we can notice, the OpenMP version fairly beats the tensorflow version.
 
----------------------------------------------------------------------------------- 
Q: Provide possible explanations for the difference in execution times. 
------------
A: A CPU is built for low latency tasks while a GPU is built for high throughput. 
   The executable generated by a compiler for CPU is optimized for minimizing 
   latency, whereas the executable generated by a GPU compiler is optimized for 
   higher throughput tasks (tasks/unit_time).
   
   Since, the installed tensorflow program generates binary optimal for a GPU, running 
   them on a CPU makes them slower.
   
   Our hypothesis is supported by  https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu
   which states that if the tensorflow programs are to be executed on a CPU then tensorflow 
   should be compiled from source with different compiler flags.
   
   
-----------------------------------------------------------------------------------
Q: Test other parameter configurations to see which of the three input 
   parameters affects execution time the most. 
  
  Be sure to include the parameter values and execution time for each test case you used.
-------------
A: 
We have analysed the execution time for both lake.py and lake.o by varying one of (N, npebs, num_iters)
while keeping the others constant.

Varying N: (npebs = 4, num_iters= 100)
While OpenMP program lake.o is faster than TensorFlow program, the execution time of both 
increase quadratically with increasing N. This is expected since, the algorithm operates on an 
N^2 grid. The graph is presented in accompanying image P2_time_vs_N.png.


We also noted that varying N on GTX1080 had comparatively less effect. The execution time grew almost linearly, and beats the lake.o program for N > 1920. The graph is in in P2_Gtx1080_VS_CPU.PNG

Varying npebs: (N=128, num_iters= 100)

Varying npebs had almost no effect on execution times. In both the cases the change in execution time looks unrelated to npebs. 
This hypothesis is based on the fact that no matter how many npebs are there the amount of calculation doesn't change. The program still has to calculate the location no reagardless of the value being zero or not.
Please note that the variation in execution time is very small.
The graph is presented in accompanying image P2_time_vs_npebs.PNG

Varying num_iters. (N=128, npebs=4)

Our guess was that varying num_iters should linearly increase the execution time of both the 
programs. The execution time of the tensorflow program increased linearly and monotonically. In case of OpenMP program, although the increase was linear it was not monotonic. We don't have an explanation for this.
Graph in accompanying image P2_time_vs_num_iter.PNG

-----------------------------------------------------------------------------------
--------------------RAW DATA ---------------------------------------------------------
Varying N:
----------
Effect on OpenMP binary(lake.o):
128     0.174701
256     0.104929
384     0.146379
512     0.189070
640     0.232693
768     0.330001
896     0.394476
1024    0.576901
1152    0.698053
1280    0.802893
1408    1.009754
1536    1.234065 
1664    1.415827
1792    1.646873
1920    1.829848

Effect on TensorFlow (lake.py):
128     0.417402983
256     0.760205984
384     1.224651814
512     1.848364115
640     2.746879816
768     3.687409878
896     4.894415855
1024    5.997572184
1152    7.436796904
1280    9.195042133
1408    10.89848995
1536    12.82465601
1664    15.00733995
1792    16.96517992
1920    19.42229414


-----------------------------------------------------------------------------------------
Varying npebs:
--------------
Effect on OpenMP binary(lake.o):
40  0.215192
60  0.046522
80  0.085503
100 0.063187
120 0.055022
140 0.084832
160 0.093367
180 0.078504
200 0.164688


Effect on TensorFlow (lake.py):
40  0.413371086
60  0.421664953
80  0.437452078
100 0.400778055
120 0.426394939
140 0.399523973
160 0.417724133
180 0.425357103
200 0.394344091
-----------------------------------------------------------------------------------------
Varying num_iter:
----------------
Effect on OpenMP binary(lake.o):
100     0.077082
200     0.112464
300     0.114738
400     0.143105
500     0.130889
600     0.109092
700     0.147709
800     0.108583
900     0.134928
1000    0.176586

Effect on TensorFlow (lake.py):

100     0.417568922
200     0.811874866
300     1.196079969
400     1.55636692
500     1.959904909
600     2.360414028
700     2.715929985
800     3.224220037
900     3.670320034
1000     3.958450079

------------------------------ EXTRA CREDIT ------------------------------------------

Comparing HW2-v2 implementation to HW6-TensorFlow implementation on GTX 1080.
 For GTX1080, We noticed that first run of lake.py would result in higher execution times than the subsequent ones.

- Varying N:
While both the execution time of both the implementations increases with increasing N. The tensorflow implementation increases almost 
linearly while our HW2-v2 implementation increase quadratically. Graph in P2_EXTRA_CREDIT_TensorFlow_vs_HW2_V2_Vary_N.PNG

- Varying npebs:
Varying npebs has no effect on the execution times. This is true for the tensorflow implementation as well as HW2-V2 implementation.
This is because, the amount of computation doesn't depend on the number of pebbles. 
However, our HW2-V2 implementation has a higher constant overhead. Hence, the absolute time taken is higher.
The graph is in P2_EXTRA_CREDIT_TensorFlow_vs_HW2_V2_Vary_NPEBS.PNG

- Varying num_iter.
Varying num_iter increased the execution time linearly. Although the rate of increase was higher in case of tensorflow.
We believe this rate is due to the different damping rate of the programs.
The graph is in P2_EXTRA_CREDIT_TensorFlow_vs_HW2_V2_Vary_num_iters.PNG

--------------- RAW DATA ----------------------------------
Varying N ------
N       TensorFlow          HW2-V2

128     2.170493841         1.13846
256     1.32942605	        1.08765
384     1.33419013	        1.071723
512     1.316764116         1.194515
640     1.367474079         1.299832
768     1.372509003         1.390176
896     1.390424013         1.454774
1024	1.409818888         1.668044
1152	1.454078197         1.867116
1280	1.491339922         2.106787
1408	1.510115862         2.475882
1536	1.586796045         2.756974
1664	1.616536856         3.229226
1792	1.692049026         3.644356
1920	1.708345175         4.242558
2048	1.752095938         5.156937
2176	1.856225014         5.660667
2304	1.881701946         6.342785
2432	2.000808001         7.427162
2560	2.059792042         8.461955

Varying npebs ------------
npebs       TensorFlow          HW2-V2
40          0.413371086	        1.035193
60          0.421664953	        1.08662
80          0.437452078	        1.115392
100         0.400778055	        1.066735
120         0.426394939	        1.038389
140         0.399523973	        1.04297
160         0.417724133	        1.059468
180         0.425357103	        1.154605
200         0.394344091	        1.021785

Varying num_iters

num_iters       TensorFlow      HW2-V2
100	            0.417568922     1.296861
200	            0.811874866     1.481751
300	            1.196079969     1.589419
400	            1.55636692	    1.836623
500	            1.959904909     1.969362
600	            2.360414028     2.106195
700	            2.715929985     2.303569
800	            3.224220037     2.427909
900	            3.670320034     2.725208
1000	        3.958450079     2.816665
