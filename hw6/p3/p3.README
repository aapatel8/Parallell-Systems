Group Author info:
aapatel8 Akshit A Patel
kmishra Kushagra Mishra
pranjan Pritesh Ranjan

-----------------------------------------------------------------------
Q. Compare the execution time of your lake-horo.py against your lake.py 
   using the parameters N=512, npebs=40, num_iter=400. 
-------------
A:
Device: GTX480. Which is capability 2 hence the program uses the CPU cores.
lake.py execution time.  7.20320606232 seconds
lake-horo.py execution time. ~ 77 seconds by each MPI task.

The problem size for one task is the same for lake.py and lake-horo.py. 
The only difference is that lake-horo has to communicate after each iteration
with the other task. This communication overhead exacerbates the run-time of 
program at each node.

Moreover, there are extra operations like slice and concatenate in each iteration
of computation. This would also be increasing the execution time.


------ RAW DATA ---------------
------ lake.py ---
[pranjan@c27 p3]$ ./lake.py 512 40 400
2017-12-10 16:05:12.393880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 480 major: 2 minor: 0 memoryClockRate(GHz): 1.401
pciBusID: 0000:03:00.0
totalMemory: 1.44GiB freeMemory: 1.39GiB
2017-12-10 16:05:12.393933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093] Ignoring visible gpu device (device: 0, name: GeForce GTX 480, pci bus id: 0000:03:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.
Elapsed time: 7.20320606232 seconds

------- lake-horo.py ----
[pranjan@c27 p3]$ mpirun -np 2 ./lake-horo.py 512 40 400
2017-12-10 16:05:28.058872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 480 major: 2 minor: 0 memoryClockRate(GHz): 1.401
pciBusID: 0000:03:00.0
totalMemory: 1.44GiB freeMemory: 1.39GiB
2017-12-10 16:05:28.058931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093] Ignoring visible gpu device (device: 0, name: GeForce GTX 480, pci bus id: 0000:03:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.
2017-12-10 16:05:28.063205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 480 major: 2 minor: 0 memoryClockRate(GHz): 1.401
pciBusID: 0000:03:00.0
totalMemory: 1.44GiB freeMemory: 1.34GiB
2017-12-10 16:05:28.063424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093] Ignoring visible gpu device (device: 0, name: GeForce GTX 480, pci bus id: 0000:03:00.0, compute capability: 2.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.
Elapsed time: 77.2116580009 seconds
Elapsed time: 77.2195849419 seconds


-------------- Extra Credit -------------------------

Comparing horovod implementation vs HW2-V3 implementation on a capability 3 GPU.
1. Varying N, while keeping npebs(=4) and num_iter(=100) constant.
----
The execution time for both the horovod and HW2-V3 implementation increase with 
increasing N. 
However, the execution time of HW2-V3 implementation increases quadratically 
while horovod execution time increases linearly.
The quadratic increase in run-time is expected because the problem size (N^2)
is also quadratic in N. However, horovod employs multiple optimizations which
keep the increase in run-time linear with respect to N.

Graph is in P3_EXTRA_CREDIT_horovod_vs_HW2-V3_VARY_N.PNG

2. Varying npebs, while keeping N(=128) and num_iter(=100) constant.
-----
As noticed with previous comparasion, npebs has no effect on the
execution time of both the versions of program. The execution time of 
both the versions remains constant with varying npebs.

Graph is in P3_EXTRA_CREDIT_horovod_vs_HW2-V3_VARY_npebs.PNG

3. Varying num_iter, while keeping N(=128) and npebs(=20) constant.
-----
The execution time of both the programs linearly increase
with increase in num_iter. This is expected because, num_iter directly 
controls the duration for which the program shall execute.

NOTE: Due to difference in the time resolutions for the programs, we scaled
the num_iter of HW2-V3 by 10. i.e. range(num_iter) = 1 to 10 (step=1) for HW2-V3
and range(num_iter) = 100 to 1000 (step=100) for horovod.

Graph is in P3_EXTRA_CREDIT_horovod_vs_HW2-V3_VARY_num_iters.PNG

-------------- RAW DATA -----------
Vary N (npebs=4, num_iter=100)
N         Horovod        HW2-V3
128     1.773118019     1.436604
256     1.715778112     1.437793
384     1.802471876     1.836287
512     1.69365406	    2.613883
640     1.781803846     4.615755
768     1.760183096     7.171418
896     1.91171813	    10.98475
1024	2.202018023     16.84471
1152	2.250592947     23.53642
1280	2.24357295	    32.219
1408	2.257546902     42.76434
1536	2.320266962     56.78825
1664	2.375886917     72.92676
1792	2.841206074     91.76101
1920	2.876095057     113.9787
2048	3.211944103     139.1663
2176	2.977270126     169.4814
2304	3.473206997     209.0183
2432	3.582471848     237.6728
2560	3.532623053     279.6247

------
Vary npebs  (N=128, num_iters=100)
npebs         Horovod        HW2-V3
40	        1.756927013	    1.436141
60	        1.778756142	    1.435091
80	        1.70330596	    1.437046
100	        1.734620094	    1.436376
120	        1.73099494	    1.436342
140	        1.693016052	    1.436314
160	        1.764686108	    1.436516
180	        1.706956863	    1.436175
200	        1.751728058	    1.436354

-------
Vary Num_iters (N=128, npebs=20)
num_iter     Horovod        HW2-V3
100	        1.814743042	    2.207091
200	        2.36345911	    3.09247
300	        2.876125097	    3.933479
400	        3.468964815	    4.758733
500	        3.98453784	    5.657725
600	        4.710052967	    6.522723
700	        5.081427097	    7.393871
800	        5.85760498	    8.268989
900	        6.693120003	    9.090983
1000        7.132858992	    9.922391
