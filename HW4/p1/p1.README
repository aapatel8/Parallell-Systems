Group Author info:
aapatel8 Akshit A Patel
kmishra Kushagra Mishra
pranjan Pritesh Ranjan

------------------------------------------------------------
-----------------------  Results Analysis ------------------
Elapsed time A:       0.24 sec
Elapsed time B:      38.46 sec
Elapsed time C:      38.47 sec
Elapsed time D:      38.49 sec


Here we have two types of parallelism 
1. Between different cores - The overhead with inter-core parallelism is due 
    of MPI communication.
2. Between different threads on the same core. - The overhead is in spawning multiple
    threads and thread scheduling on the same core.
    
Limits of parallelism: 
1. If we have tasks which are computation heavy 
(i.e. Computation/ Communication ratio is high) then having many threads on the same 
core is not helpful. In such cases having mupltiple cores/nodes is beneficial.

2. If we have tasks which is inherently sequential (i.e. lots of synchronization 
is required among threads), then even having multiple cores is not beneficial.

---------------------------------------------------------------------------
Case A: (X,T,P,S,I) = (1,8,8,20,5) -- Combination of MPI and OpenMP
We have 8 MPI tasks with 8 OpenMP threads spawned by each task and we have
16 cores available. The MPI tasks are bound to a core and the 8 OpenMP threads 
are associated with the task, OpenMP threads take turns in running on the core. 

Elapsed time = 0.24 Sec.
% of Total time spent by all processes in MPI : 18.12%
---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0      0.281      0.021     7.48
   1      0.259     0.0386    14.87
   2      0.256     0.0562    21.96
   3      0.253     0.0536    21.15
   4      0.255     0.0553    21.73
   5      0.251     0.0539    21.44
   6      0.255     0.0403    15.84
   7      0.251     0.0547    21.78
   *       2.06      0.374    18.12

MPI task 0 has a longer AppTime than other tasks because it does some extra book-keeping. 
Task 0 spends less time in MPI.
Most expensive MPI Call: Allreduce    App% = 9.56%  MPI%= 52.74%
The Allreduce function calls used 9.56% of the total aggregate application time and
52.74% of total aggregate MPI time.

We observed that with increasing number of OpenMP threads 
(T=1,8,16,64) the elapsed time increased with increasing number of OpenMP threads. 
This is because the Lulesh job is computation heavy and we don't get much benefit by
 having multiple OpenMp threads on the same core. The thread spawning and scheduling overhead 
 increased with number of threads and hence increased the elapsed time.

We also observed that keeping T constant, but increasing the number of cores decreased 
the app-time but increased the MPI% time.
This follows from previous hypothesis that with more number of cores the computation
is divided by different tasks decreasing the App time but increases the MPI 
communication.

Increasing problem size(P) with keeping all other factors increased the elapsed time,
but not significantly (sub-linear increase).

---------------------------------------------------------------------------
Case B: (X,T,P,S,I) = (1,8,8,20,5) Same as A, but without core binding
We have 8 MPI tasks with 8 OpenMP threads spawned by each task and we have
16 cores available. An MPI task shall be pinned to one core but the OpenMP threads
can be scheduled on different cores.

Elapsed time = 38.46 (s)
% of Total time spent by all processes in MPI : 0.35%

All nodes take almost the same amount of AppTime and MPITime in Case B. This is because
the book-keeping overhead at Node 0 becomes irrelevant compared to AppTime.

Most expensive MPI Call: Allreduce    App% = 0.10%  MPI%= 29.49%

We observed that elapsed time increased drastically due to removing 
-bind-to-core option for OpenMP threads while keeping all other factors constant.
This is because, when the OpenMP threads are dynamically scheduled on different cores,
there is an overhead associated with thread schedulig, and cache warm up on the new core.

This hypothesis is supported by the fact that although absolute MPITime remained 
almost same, AppTime increased drastically. In other words, time spent outside of 
MPI calls increased. Since all other arguments are constant, this increase in AppTime
must be due to the thread scheduling overhead on different cores. Moreover, this overhead
increased with increasing number of threads (T).

---------------------------------------------------------------------------
Case C: (X,T,P,S,I) = (4,0,64,80,20) -- MPI Only

Compared to case A, we have more cores i.e. 64 and more MPI tasks i.e. 64 a bigger
problem size and we are running for more iterations.  

Elapsed time         =      38.47 (s)
% of Total time spent by all processes in MPI : 11%

Most expensive MPI Call: Allreduce    App% = 10.26%  MPI%= 92.70%
We notice that most of the MPI time is spent in reducing the results from different tasks.
This is expected because we have larger problem size which will result in larger intermediate
values. Moreover, we have large number of nodes to reduce data from, hence Allreduce 
takes the lion chunk of MPI Time.

As expected, the absolute MPITime increases for all tasks, because we have more tasks.
The app-time also increases (compared to A), which is also as expected because the 
problem size. However, time spent in MPI_Wait call doesn't take such a big chunk of time.

---------------------------------------------------------------------------
Case D: (X,T,P,S,I) = (4,0,64,80,20) -- Same as C but without core binding

Compared to case C, we don't observe any significant difference in timing stats.
This is expected because we haven't changed any argument. 
The only difference is that we are not binding OpenMP threads to a core. Since, we 
don't have any OpenMP thread, this option doesn't make a difference.

Elapsed time         =      38.49 (s)
% of Total time spent by all processes in MPI : 11.36%

Most expensive MPI Call: Allreduce    App% = 10.21%  MPI%= 89.92%
  



--------------------------Relevant Raw Output -----------------------------
---------------------------------------------------------------------------
Case A: (X,T,P,S,I) = (1,8,8,20,5) -- Combination of MPI and OpenMP
---------------------------------------------------------------------------

[kmishra@c47 lulesh2.0.3]$ mpirun -np 8 -bind-to core ./lulesh2.0 -s 20 -i 5
Warning: Process to core binding is enabled and OMP_NUM_THREADS is set to non-zero (8) value
If your program has OpenMP sections, this can cause over-subscription of cores and consequently poor performance
To avoid this, please re-run your application after setting MV2_ENABLE_AFFINITY=0
Use MV2_USE_THREAD_WARNING=0 to suppress this message
mpiP:
mpiP: mpiP: mpiP V3.4.1 (Build Oct 21 2017/20:38:56)
mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net
mpiP:
Running problem size 20^3 per domain until completion
Num processors: 8
Num threads: 8
Total number of elements: 64000

To run other sizes, use -s <integer>.
To run a fixed number of iterations, use -i <integer>.
To run a more or less balanced region set, use -b <integer>.
To change the relative costs of regions, use -c <integer>.
To print out progress, use -p
To write an output file for VisIt, use -v
See help (-h) for more options

Run completed:
   Problem size        =  20
   MPI tasks           =  8
   Iteration count     =  5
   Final Origin Energy = 2.092107e+07
   Testing Plane 0 of Energy Array on rank 0:
        MaxAbsDiff   = 6.548362e-11
        TotalAbsDiff = 6.559739e-11
        MaxRelDiff   = 8.066288e-13


Elapsed time         =       0.24 (s)
Grind time (us/z/c)  =  5.9339941 (per dom)  (0.74174926 overall)
FOM                  =  1348.1645 (z/s)

---------------------------------------------------------------------------
mpiP:
mpiP: Storing mpiP output in [./lulesh2.0.8.22519.1.mpiP].
mpiP:

@ mpiP
@ Command : ./lulesh2.0 -s 20 -i 5 
@ Version                  : 3.4.1
@ MPIP Build date          : Oct 21 2017, 20:38:56
@ Start time               : 2017 10 21 20:47:44
@ Stop time                : 2017 10 21 20:47:44
@ Timer Used               : PMPI_Wtime
@ MPIP env var             : [null]
@ Collector Rank           : 0
@ Collector PID            : 22519
@ Final Output Dir         : .
@ Report generation        : Single collector task
@ MPI Task Assignment      : 0 c47
@ MPI Task Assignment      : 1 c47
@ MPI Task Assignment      : 2 c47
@ MPI Task Assignment      : 3 c47
@ MPI Task Assignment      : 4 c47
@ MPI Task Assignment      : 5 c47
@ MPI Task Assignment      : 6 c47
@ MPI Task Assignment      : 7 c47

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0      0.281      0.021     7.48
   1      0.259     0.0386    14.87
   2      0.256     0.0562    21.96
   3      0.253     0.0536    21.15
   4      0.255     0.0553    21.73
   5      0.251     0.0539    21.44
   6      0.255     0.0403    15.84
   7      0.251     0.0547    21.78
   *       2.06      0.374    18.12

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site       Time    App%    MPI%     COV
Allreduce              12        197    9.56   52.74    0.50
Irecv                  28       67.4    3.27   18.04    0.09
Irecv                  78       59.4    2.88   15.91    0.01
Wait                    1       8.03    0.39    2.15    0.65
Barrier                15       6.89    0.33    1.85    0.06
Isend                  74       3.89    0.19    1.04    0.04
Wait                   84       3.58    0.17    0.96    1.21
Isend                   3       3.17    0.15    0.85    0.07
Reduce                 14       2.79    0.14    0.75    2.32
Waitall                19       2.32    0.11    0.62    0.32
Wait                   83       1.66    0.08    0.44    0.94
Wait                   10       1.35    0.07    0.36    1.45
Isend                  50       1.11    0.05    0.30    0.14
Wait                   34       1.11    0.05    0.30    0.57
Isend                  65       1.09    0.05    0.29    0.02
Wait                   17      0.963    0.05    0.26    1.32
Isend                  33      0.793    0.04    0.21    0.20
Isend                  35      0.776    0.04    0.21    0.32
Wait                   23      0.632    0.03    0.17    0.95
Wait                   30      0.536    0.03    0.14    0.11
   
-------------------------------------------------------------------------------
Case B: (X,T,P,S,I) = (1,8,8,20,5) Same as A, but without core binding
-------------------------------------------------------------------------------

[kmishra@c47 lulesh2.0.3]$ mpirun -np 8 ./lulesh2.0 -s 20 -i 5
Warning: Process to core binding is enabled and OMP_NUM_THREADS is set to non-zero (8) value
If your program has OpenMP sections, this can cause over-subscription of cores and consequently poor performance
To avoid this, please re-run your application after setting MV2_ENABLE_AFFINITY=0
Use MV2_USE_THREAD_WARNING=0 to suppress this message
mpiP:
mpiP: mpiP: mpiP V3.4.1 (Build Oct 21 2017/20:38:56)
mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net
mpiP:
Running problem size 20^3 per domain until completion
Num processors: 8
Num threads: 8
Total number of elements: 64000

To run other sizes, use -s <integer>.
To run a fixed number of iterations, use -i <integer>.
To run a more or less balanced region set, use -b <integer>.
To change the relative costs of regions, use -c <integer>.
To print out progress, use -p
To write an output file for VisIt, use -v
See help (-h) for more options

Run completed:
   Problem size        =  20
   MPI tasks           =  8
   Iteration count     =  5
   Final Origin Energy = 2.092107e+07
   Testing Plane 0 of Energy Array on rank 0:
        MaxAbsDiff   = 6.548362e-11
        TotalAbsDiff = 6.559739e-11
        MaxRelDiff   = 8.066288e-13


Elapsed time         =      38.46 (s)
Grind time (us/z/c)  =  961.55721 (per dom)  ( 120.19465 overall)
FOM                  =  8.3198378 (z/s)

mpiP:
mpiP: Storing mpiP output in [./lulesh2.0.8.22618.1.mpiP].
mpiP:
---------------------------------------------------------------------------

@ mpiP
@ Command : ./lulesh2.0 -s 20 -i 5 
@ Version                  : 3.4.1
@ MPIP Build date          : Oct 21 2017, 20:38:56
@ Start time               : 2017 10 21 20:49:24
@ Stop time                : 2017 10 21 20:50:03
@ Timer Used               : PMPI_Wtime
@ MPIP env var             : [null]
@ Collector Rank           : 0
@ Collector PID            : 22618
@ Final Output Dir         : .
@ Report generation        : Single collector task
@ MPI Task Assignment      : 0 c47
@ MPI Task Assignment      : 1 c47
@ MPI Task Assignment      : 2 c47
@ MPI Task Assignment      : 3 c47
@ MPI Task Assignment      : 4 c47
@ MPI Task Assignment      : 5 c47
@ MPI Task Assignment      : 6 c47
@ MPI Task Assignment      : 7 c47

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0       38.5      0.138     0.36
   1       38.5      0.138     0.36
   2       38.5      0.152     0.39
   3       38.5      0.126     0.33
   4       38.5       0.16     0.41
   5       38.5      0.153     0.40
   6       38.5      0.108     0.28
   7       38.5      0.113     0.29
   *        308       1.09     0.35

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site       Time    App%    MPI%     COV
Allreduce              12        321    0.10   29.49    0.35
Wait                   82        120    0.04   10.98    0.18
Wait                    1        116    0.04   10.63    0.82
Wait                   10       68.3    0.02    6.28    0.39
Wait                   84       67.2    0.02    6.18    0.25
Wait                   31       60.5    0.02    5.56    0.59
Wait                   30       38.2    0.01    3.51    0.92
Wait                   23       31.6    0.01    2.91    1.17
Wait                   34       30.2    0.01    2.77    0.89
Wait                    2       26.4    0.01    2.43    0.41
Wait                   54         19    0.01    1.75    0.86
Wait                   47       18.8    0.01    1.72    1.16
Wait                   17       14.2    0.00    1.31    0.20
Wait                   22       10.9    0.00    1.00    1.71
Wait                   21        9.6    0.00    0.88    1.98
Wait                   83       9.38    0.00    0.86    1.30
Wait                   13       9.38    0.00    0.86    1.62
Wait                   92       8.83    0.00    0.81    1.41
Wait                   49       8.76    0.00    0.81    0.13
Wait                   90       8.41    0.00    0.77    0.00
   
-------------------------------------------------------------------------------
Case C: (X,T,P,S,I) = (4,0,64,80,20) -- MPI Only
-------------------------------------------------------------------------------

[kmishra@c74 lulesh2.0.3]$ mpirun -np 64 -bind-to core ./lulesh2.0 -s 80 -i 20
mpiP:
mpiP: mpiP: mpiP V3.4.1 (Build Oct 21 2017/20:52:57)
mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net
mpiP:
Running problem size 80^3 per domain until completion
Num processors: 64
Total number of elements: 32768000

To run other sizes, use -s <integer>.
To run a fixed number of iterations, use -i <integer>.
To run a more or less balanced region set, use -b <integer>.
To change the relative costs of regions, use -c <integer>.
To print out progress, use -p
To write an output file for VisIt, use -v
See help (-h) for more options

Run completed:
   Problem size        =  80
   MPI tasks           =  64
   Iteration count     =  20
   Final Origin Energy = 5.316142e+09
   Testing Plane 0 of Energy Array on rank 0:
        MaxAbsDiff   = 2.980232e-07
        TotalAbsDiff = 3.026954e-07
        MaxRelDiff   = 1.633415e-12


Elapsed time         =      38.47 (s)
Grind time (us/z/c)  =  3.7567064 (per dom)  (0.058698537 overall)
FOM                  =    17036.2 (z/s)

mpiP:
mpiP: Storing mpiP output in [./lulesh2.0.64.21675.1.mpiP].
mpiP:

-------------------------------------------------------------------------------
@ mpiP
@ Command : ./lulesh2.0 -s 80 -i 20 
@ Version                  : 3.4.1
@ MPIP Build date          : Oct 21 2017, 20:52:57
@ Start time               : 2017 10 21 21:08:13
@ Stop time                : 2017 10 21 21:08:52
@ Timer Used               : PMPI_Wtime
@ MPIP env var             : [null]
@ Collector Rank           : 0
@ Collector PID            : 21675
@ Final Output Dir         : .
@ Report generation        : Single collector task
@ MPI Task Assignment      : 0 c74
@ MPI Task Assignment      : 1 c74
@ MPI Task Assignment      : 2 c74
@ MPI Task Assignment      : 3 c74
@ MPI Task Assignment      : 4 c74
@ MPI Task Assignment      : 5 c74
@ MPI Task Assignment      : 6 c74
@ MPI Task Assignment      : 7 c74
@ MPI Task Assignment      : 8 c74
@ MPI Task Assignment      : 9 c74
@ MPI Task Assignment      : 10 c74
@ MPI Task Assignment      : 11 c74
@ MPI Task Assignment      : 12 c74
@ MPI Task Assignment      : 13 c74
@ MPI Task Assignment      : 14 c74
@ MPI Task Assignment      : 15 c74
@ MPI Task Assignment      : 16 c75
@ MPI Task Assignment      : 17 c75
@ MPI Task Assignment      : 18 c75
@ MPI Task Assignment      : 19 c75
@ MPI Task Assignment      : 20 c75
@ MPI Task Assignment      : 21 c75
@ MPI Task Assignment      : 22 c75
@ MPI Task Assignment      : 23 c75
@ MPI Task Assignment      : 24 c75
@ MPI Task Assignment      : 25 c75
@ MPI Task Assignment      : 26 c75
@ MPI Task Assignment      : 27 c75
@ MPI Task Assignment      : 28 c75
@ MPI Task Assignment      : 29 c75
@ MPI Task Assignment      : 30 c75
@ MPI Task Assignment      : 31 c75
@ MPI Task Assignment      : 32 c76
@ MPI Task Assignment      : 33 c76
@ MPI Task Assignment      : 34 c76
@ MPI Task Assignment      : 35 c76
@ MPI Task Assignment      : 36 c76
@ MPI Task Assignment      : 37 c76
@ MPI Task Assignment      : 38 c76
@ MPI Task Assignment      : 39 c76
@ MPI Task Assignment      : 40 c76
@ MPI Task Assignment      : 41 c76
@ MPI Task Assignment      : 42 c76
@ MPI Task Assignment      : 43 c76
@ MPI Task Assignment      : 44 c76
@ MPI Task Assignment      : 45 c76
@ MPI Task Assignment      : 46 c76
@ MPI Task Assignment      : 47 c76
@ MPI Task Assignment      : 48 c77
@ MPI Task Assignment      : 49 c77
@ MPI Task Assignment      : 50 c77
@ MPI Task Assignment      : 51 c77
@ MPI Task Assignment      : 52 c77
@ MPI Task Assignment      : 53 c77
@ MPI Task Assignment      : 54 c77
@ MPI Task Assignment      : 55 c77
@ MPI Task Assignment      : 56 c77
@ MPI Task Assignment      : 57 c77
@ MPI Task Assignment      : 58 c77
@ MPI Task Assignment      : 59 c77
@ MPI Task Assignment      : 60 c77
@ MPI Task Assignment      : 61 c77
@ MPI Task Assignment      : 62 c77
@ MPI Task Assignment      : 63 c77

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0       38.8      0.607     1.57
   1       38.7      0.333     0.86
   2       38.6       2.76     7.14
   3       38.6        2.9     7.52
   4       38.5       4.44    11.53
   5       38.5       4.99    12.96
   6       38.5        5.2    13.50
   7       38.5       5.44    14.14
   8       38.6       6.33    16.41
   9       38.4       6.85    17.83
  10       38.4       7.03    18.30
  11       38.6       3.15     8.17
  12       38.7        1.7     4.40
  13       38.7       2.04     5.28
  14       38.6          3     7.77
  15       38.6       3.58     9.27
  16       38.7       4.11    10.62
  17       38.5        5.1    13.26
  18       38.5       5.11    13.27
  19       38.5       5.15    13.37
  20       38.7       7.34    18.97
  21       38.4       6.74    17.53
  22       38.7       1.58     4.08
  23       38.6       2.88     7.46
  24       38.7       2.05     5.31
  25       38.6        2.7     7.00
  26       38.6       3.56     9.22
  27       38.5       4.94    12.83
  28       38.5       5.47    14.23
  29       38.4       6.22    16.18
  30       38.4       6.79    17.66
  31       38.4       6.77    17.61
  32       38.7       7.43    19.17
  33       38.7       1.87     4.84
  34       38.7       1.62     4.18
  35       38.6       3.04     7.87
  36       38.7       1.46     3.77
  37       38.6       3.02     7.81
  38       38.6       3.72     9.65
  39       38.5       4.88    12.67
  40       38.4       6.28    16.34
  41       38.4       6.69    17.42
  42       38.4       6.59    17.16
  43       38.4       7.32    19.08
  44       38.7      0.612     1.58
  45       38.8      0.276     0.71
  46       38.6       2.32     6.01
  47       38.7       1.98     5.12
  48       38.7       3.09     7.97
  49       38.5       5.01    13.01
  50       38.5       5.41    14.04
  51       38.5        4.3    11.15
  52       38.6       7.35    19.05
  53       38.4       7.24    18.86
  54       38.4       7.33    19.10
  55       38.6        3.6     9.34
  56       38.7       1.86     4.82
  57       38.7       2.02     5.22
  58       38.6       2.85     7.37
  59       38.6       2.93     7.59
  60       38.5       5.51    14.32
  61       38.5       5.97    15.54
  62       38.5       5.99    15.59
  63       38.4       6.86    17.87
   *   2.47e+03        273    11.07

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site       Time    App%    MPI%     COV
Allreduce              22   2.53e+05   10.26   92.70    0.52
Waitall                31   1.07e+04    0.44    3.93    0.30
Reduce                 30   1.35e+03    0.05    0.50    3.23
Barrier                 5   1.26e+03    0.05    0.46    0.11
Wait                   14        725    0.03    0.27    1.13
Isend                  59        305    0.01    0.11    0.52
Isend                  82        262    0.01    0.10    0.06
Isend                  23        251    0.01    0.09    0.45
Wait                   78        234    0.01    0.09    1.11
Wait                   65        210    0.01    0.08    0.70
Wait                   67        205    0.01    0.08    1.27
Wait                   56        194    0.01    0.07    1.03
Isend                  43        193    0.01    0.07    0.34
Wait                   97        188    0.01    0.07    1.22
Wait                   16        179    0.01    0.07    0.91
Wait                    1        178    0.01    0.07    1.11
Wait                   44        157    0.01    0.06    1.40
Irecv                   2        150    0.01    0.05    1.25
Wait                   58        146    0.01    0.05    1.59
Wait                   15        135    0.01    0.05    1.27
   
-------------------------------------------------------------------------------
Case D: (X,T,P,S,I) = (4,0,64,80,20) -- Same as C but without core binding
-------------------------------------------------------------------------------

[kmishra@c74 lulesh2.0.3]$ mpirun -np 64 ./lulesh2.0 -s 80 -i 20
mpiP:
mpiP: mpiP: mpiP V3.4.1 (Build Oct 21 2017/20:52:57)
mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net
mpiP:
Running problem size 80^3 per domain until completion
Num processors: 64
Total number of elements: 32768000

To run other sizes, use -s <integer>.
To run a fixed number of iterations, use -i <integer>.
To run a more or less balanced region set, use -b <integer>.
To change the relative costs of regions, use -c <integer>.
To print out progress, use -p
To write an output file for VisIt, use -v
See help (-h) for more options

Run completed:
   Problem size        =  80
   MPI tasks           =  64
   Iteration count     =  20
   Final Origin Energy = 5.316142e+09
   Testing Plane 0 of Energy Array on rank 0:
        MaxAbsDiff   = 2.980232e-07
        TotalAbsDiff = 3.026954e-07
        MaxRelDiff   = 1.633415e-12


Elapsed time         =      38.49 (s)
Grind time (us/z/c)  =  3.7586838 (per dom)  (0.058729435 overall)
FOM                  =  17027.237 (z/s)

mpiP:
mpiP: Storing mpiP output in [./lulesh2.0.64.21782.1.mpiP].
mpiP:

---------------------------------------------------------------------------------
@ mpiP
@ Command : ./lulesh2.0 -s 80 -i 20 
@ Version                  : 3.4.1
@ MPIP Build date          : Oct 21 2017, 20:52:57
@ Start time               : 2017 10 21 21:09:37
@ Stop time                : 2017 10 21 21:10:16
@ Timer Used               : PMPI_Wtime
@ MPIP env var             : [null]
@ Collector Rank           : 0
@ Collector PID            : 21782
@ Final Output Dir         : .
@ Report generation        : Single collector task
@ MPI Task Assignment      : 0 c74
@ MPI Task Assignment      : 1 c74
@ MPI Task Assignment      : 2 c74
@ MPI Task Assignment      : 3 c74
@ MPI Task Assignment      : 4 c74
@ MPI Task Assignment      : 5 c74
@ MPI Task Assignment      : 6 c74
@ MPI Task Assignment      : 7 c74
@ MPI Task Assignment      : 8 c74
@ MPI Task Assignment      : 9 c74
@ MPI Task Assignment      : 10 c74
@ MPI Task Assignment      : 11 c74
@ MPI Task Assignment      : 12 c74
@ MPI Task Assignment      : 13 c74
@ MPI Task Assignment      : 14 c74
@ MPI Task Assignment      : 15 c74
@ MPI Task Assignment      : 16 c75
@ MPI Task Assignment      : 17 c75
@ MPI Task Assignment      : 18 c75
@ MPI Task Assignment      : 19 c75
@ MPI Task Assignment      : 20 c75
@ MPI Task Assignment      : 21 c75
@ MPI Task Assignment      : 22 c75
@ MPI Task Assignment      : 23 c75
@ MPI Task Assignment      : 24 c75
@ MPI Task Assignment      : 25 c75
@ MPI Task Assignment      : 26 c75
@ MPI Task Assignment      : 27 c75
@ MPI Task Assignment      : 28 c75
@ MPI Task Assignment      : 29 c75
@ MPI Task Assignment      : 30 c75
@ MPI Task Assignment      : 31 c75
@ MPI Task Assignment      : 32 c76
@ MPI Task Assignment      : 33 c76
@ MPI Task Assignment      : 34 c76
@ MPI Task Assignment      : 35 c76
@ MPI Task Assignment      : 36 c76
@ MPI Task Assignment      : 37 c76
@ MPI Task Assignment      : 38 c76
@ MPI Task Assignment      : 39 c76
@ MPI Task Assignment      : 40 c76
@ MPI Task Assignment      : 41 c76
@ MPI Task Assignment      : 42 c76
@ MPI Task Assignment      : 43 c76
@ MPI Task Assignment      : 44 c76
@ MPI Task Assignment      : 45 c76
@ MPI Task Assignment      : 46 c76
@ MPI Task Assignment      : 47 c76
@ MPI Task Assignment      : 48 c77
@ MPI Task Assignment      : 49 c77
@ MPI Task Assignment      : 50 c77
@ MPI Task Assignment      : 51 c77
@ MPI Task Assignment      : 52 c77
@ MPI Task Assignment      : 53 c77
@ MPI Task Assignment      : 54 c77
@ MPI Task Assignment      : 55 c77
@ MPI Task Assignment      : 56 c77
@ MPI Task Assignment      : 57 c77
@ MPI Task Assignment      : 58 c77
@ MPI Task Assignment      : 59 c77
@ MPI Task Assignment      : 60 c77
@ MPI Task Assignment      : 61 c77
@ MPI Task Assignment      : 62 c77
@ MPI Task Assignment      : 63 c77

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0         39      0.727     1.87
   1       38.9      0.586     1.50
   2       38.7       3.03     7.82
   3       38.7       3.05     7.87
   4       38.6       4.53    11.74
   5       38.7       5.34    13.80
   6       38.6       5.42    14.03
   7       38.6       5.49    14.24
   8       38.8       6.45    16.64
   9       38.5       6.93    18.03
  10       38.6       7.25    18.79
  11       38.6       3.09     8.00
  12       38.8       2.08     5.36
  13       38.9       2.39     6.15
  14       38.7       3.04     7.86
  15       38.6       3.53     9.13
  16       38.8       4.34    11.19
  17       38.6       5.21    13.49
  18       38.6       5.09    13.19
  19       38.6       5.34    13.83
  20       38.7       7.53    19.43
  21       38.6       7.05    18.27
  22       38.8       1.73     4.46
  23       38.8       3.13     8.07
  24       38.7       1.96     5.06
  25       38.7       2.68     6.94
  26       38.8       3.67     9.47
  27       38.6       4.84    12.54
  28       38.6       5.74    14.86
  29       38.6        6.3    16.31
  30       38.5       6.77    17.60
  31       38.6       6.91    17.93
  32       38.8       7.41    19.12
  33       38.7        1.8     4.65
  34       38.7       1.61     4.17
  35       38.6       3.12     8.07
  36       38.7       1.69     4.37
  37       38.7       3.27     8.44
  38       38.8       4.19    10.81
  39       38.7       5.36    13.84
  40       38.5       6.42    16.65
  41       38.5        6.6    17.16
  42       38.4       6.55    17.04
  43       38.4       7.35    19.14
  44       38.9      0.884     2.27
  45       38.8      0.464     1.20
  46       38.6       2.47     6.40
  47       38.8       2.16     5.57
  48       38.7       3.13     8.08
  49       38.5       5.09    13.21
  50       38.5       5.35    13.91
  51       38.5       4.27    11.09
  52       38.6        7.4    19.18
  53       38.5       7.54    19.57
  54       38.5       7.47    19.38
  55       38.6       3.63     9.40
  56       38.8       1.93     4.98
  57       38.6       2.02     5.22
  58       38.6          3     7.76
  59       38.7       3.12     8.05
  60       38.5       5.42    14.08
  61       38.4       5.98    15.55
  62       38.6       6.14    15.91
  63       38.5       6.94    18.02
   *   2.47e+03        281    11.36

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site       Time    App%    MPI%     COV
Allreduce              22   2.53e+05   10.21   89.92    0.53
Waitall                31   1.64e+04    0.66    5.84    0.37
Barrier                 5   2.27e+03    0.09    0.81    1.00
Reduce                 30   1.36e+03    0.06    0.48    3.24
Wait                   72        588    0.02    0.21    1.61
Wait                   14        534    0.02    0.19    1.12
Wait                   16        408    0.02    0.15    0.87
Wait                   61        342    0.01    0.12    1.68
Isend                  63        326    0.01    0.12    0.49
Isend                  86        325    0.01    0.12    0.10
Wait                   83        299    0.01    0.11    0.95
Wait                   21        293    0.01    0.10    1.91
Wait                    1        285    0.01    0.10    1.34
Isend                  23        269    0.01    0.10    0.43
Wait                   12        266    0.01    0.09    1.58
Wait                   70        251    0.01    0.09    1.02
Wait                   58        215    0.01    0.08    1.20
Isend                  43        214    0.01    0.08    0.29
Wait                   50        182    0.01    0.06    1.60
Wait                   53        178    0.01    0.06    1.39
---------------------------------------------------------------------------
   
